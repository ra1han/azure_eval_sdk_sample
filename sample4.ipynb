{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c21eb19",
   "metadata": {},
   "source": [
    "# Sample 4\n",
    "\n",
    "Demonstrates how to use Azure Eval SDK to run evaluations in the cloud using AI Foundry Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a588d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "endpoint = os.environ[\"FOUNDRY_PROJECT_ENDPOINT\"] # https://<account>.services.ai.azure.com/api/projects/<project>\n",
    "model_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"] # https://<account>.services.ai.azure.com\n",
    "model_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "model_deployment_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    "\n",
    "# Optional â€“ reuse an existing dataset\n",
    "dataset_name    = os.environ.get(\"DATASET_NAME\",    \"dataset-test\")\n",
    "dataset_version = os.environ.get(\"DATASET_VERSION\", \"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583d068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c726c",
   "metadata": {},
   "source": [
    "### Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c37454",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = project_client.datasets.upload_file(\n",
    "    name=dataset_name,\n",
    "    version=dataset_version,\n",
    "    file_path=\"./data/qna.jsonl\",\n",
    ").id\n",
    "# 'azureai://accounts/<project_name>-resource/projects/<project_name>/data/<dataset_name>/versions/1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b86e5",
   "metadata": {},
   "source": [
    "### Configure evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0776efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "\n",
    "# Built-in evaluator configurations\n",
    "evaluators = {\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.question}\",\n",
    "            \"response\": \"${data.answer}\",\n",
    "        },\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.VIOLENCE.value,\n",
    "        init_params={\"azure_ai_project\": endpoint},\n",
    "    ),\n",
    "    \"bleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.BLEU_SCORE.value,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ef22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    InputDataset\n",
    ")\n",
    "\n",
    "# Create an evaluation with the dataset and evaluators specified\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=InputDataset(id=data_id),\n",
    "    evaluators=evaluators,\n",
    ")\n",
    "\n",
    "# Run the evaluation \n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation,\n",
    "    headers={\n",
    "        \"model-endpoint\": model_endpoint,\n",
    "        \"api-key\": model_api_key,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Created evaluation:\", evaluation_response.name)\n",
    "print(\"Status:\", evaluation_response.status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
