{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c21eb19",
   "metadata": {},
   "source": [
    "# Sample 3\n",
    "\n",
    "This notebook demonstrate how to use Azure Eval SDK to run evaluations in the cloud (AI Foundry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "from custom_evaluator.friendliness_evaluator import FriendlinessEvaluator\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Model\n",
    "from promptflow.client import PFClient\n",
    "\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    Dataset,\n",
    "    EvaluatorConfiguration,\n",
    "    ConnectionType\n",
    ")\n",
    "from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - to support DefaultAzureCredential()\n",
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17698c25",
   "metadata": {},
   "source": [
    "### Upload dataset to AI Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e83ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"AI_FOUNDRY_CONNECTION_STRING\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.abspath(\"data/qna.jsonl\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")\n",
    "        print(f\"File size: {os.path.getsize(file_path)} bytes\")\n",
    "        data_id, _ = project_client.upload_file(file_path)\n",
    "        print(f\"Data ID: {data_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479142e",
   "metadata": {},
   "source": [
    "### Register the custom evaluator into AI Foundry\n",
    "Steps \n",
    "- Convert the evaluator into an evaluation flow and save locally \n",
    "- Register it to AI Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4268cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(\n",
    "       subscription_id=os.environ[\"SUBSCRIPTION_ID\"],\n",
    "       resource_group_name=os.environ[\"RESOURCE_GROUP\"],\n",
    "       workspace_name=os.environ[\"AI_FOUNDRY_PROJECT_NAME\"],\n",
    "       credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Create flow from the custom evaluator\n",
    "local_path = \"dist/FriendlinessEvaluator\" # save this evaluation flow in /dist\n",
    "pf_client = PFClient()\n",
    "pf_client.flows.save(entry=FriendlinessEvaluator, path=local_path)\n",
    "\n",
    "evaluator_name = \"FriendlinessEvaluator\"\n",
    "custom_evaluator = Model(\n",
    "    path=local_path,\n",
    "    name=evaluator_name,\n",
    "    description=\"custom prompt evaluator measuring response base on friendliness\",\n",
    ")\n",
    "\n",
    "# Register the custom evaluator\n",
    "registered_evaluator = ml_client.evaluators.create_or_update(custom_evaluator)\n",
    "print(\"Registered evaluator id:\", registered_evaluator.id)\n",
    "\n",
    "versioned_evaluator = ml_client.evaluators.get(evaluator_name, label=\"latest\")\n",
    "print(\"Versioned evaluator id:\", versioned_evaluator.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55650d0a",
   "metadata": {},
   "source": [
    "### Programatically construct custom evaluator id\n",
    "\n",
    "Currently when we register the custom evaluator into AI Foundry, the `registered_evaluator.id` variable returns the resource id (https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.job?view=azure-python) which is not in the format of the required asset ID in the parameter for `EvaluatorConfiguration` class. \n",
    "So there is two options to retrieve this asset ID:\n",
    "- We can manually go into the AI Foundry Evaluator Library and copy the asset ID via UI.\n",
    "- The format for the asset ID currently is `azureml://locations/{location}/workspaces/{project_id}/models/{evaluator_name}/versions/{versioned_evaluator.version}` so we can programmatically retrieve the project_id, evaluator_name and version of latest evaluator and construct the asset id for the custom evaluator. We will need to use `azure.mgmt.machinelearningservices` to retrieve the location and project_id that constructs the asset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_connection = project_client.connections.get_default(connection_type=ConnectionType.AZURE_OPEN_AI)\n",
    "\n",
    "# Create model configuration for evaluators that need a model\n",
    "model_config = default_connection.to_evaluator_model_config(\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "subscription_id = os.environ[\"SUBSCRIPTION_ID\"]\n",
    "\n",
    "ml_client = AzureMachineLearningWorkspaces(credential, subscription_id)\n",
    "\n",
    "resource_group_name = os.environ[\"RESOURCE_GROUP\"]\n",
    "workspace_name = os.environ[\"AI_FOUNDRY_PROJECT_NAME\"]\n",
    "\n",
    "workspace = ml_client.workspaces.get(resource_group_name, workspace_name)\n",
    "\n",
    "project_id = workspace.workspace_id\n",
    "location = workspace.location\n",
    "\n",
    "custom_evaluator_id = f\"azureml://locations/{location}/workspaces/{project_id}/models/{evaluator_name}/versions/{versioned_evaluator.version}\"\n",
    "print(custom_evaluator_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcbd23",
   "metadata": {},
   "source": [
    "### Create evaluation using published custom evaluator library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud Evaluation 2\",\n",
    "    description=\"Cloud Evaluation\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators={\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=RelevanceEvaluator.id, # in-built evaluator id\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.question}\",\n",
    "                \"response\": \"${data.answer}\",\n",
    "            }\n",
    "        ),\n",
    "        \"friendliness\": EvaluatorConfiguration(\n",
    "            id=custom_evaluator_id, # custom evaluator id\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "            data_mapping={\n",
    "                \"response\": \"${data.answer}\",\n",
    "            }\n",
    "        ),\n",
    "\n",
    "    },\n",
    ")\n",
    "\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation=evaluation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalsdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
